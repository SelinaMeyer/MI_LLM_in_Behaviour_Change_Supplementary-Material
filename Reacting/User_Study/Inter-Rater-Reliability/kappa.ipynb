{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations of unsuitable bot behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "annos_rater_1 = pd.read_csv(\"bot_behaviour/annotation_Round_2_Rater_1.csv\")\n",
    "annos_rater_1.fillna(0, inplace=True)\n",
    "annos_rater_2 = pd.read_csv(\"bot_behaviour/Annotations_Round_2_Rater_2.csv\")\n",
    "annos_round_1 = pd.read_csv(\"bot_behaviour/Annotations_Round_1.csv\")\n",
    "annos_round_1.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1:  0.30441898527004907\n",
      "Round 2:  0.5411764705882353\n"
     ]
    }
   ],
   "source": [
    "print(\"Round 1: \", cohen_kappa_score(annos_round_1['Giving Facts Rater 1'], annos_round_1['Giving Facts Rater 2']))\n",
    "print(\"Round 2: \", cohen_kappa_score(annos_rater_1[\"Giving Facts\"], annos_rater_2[\"Giving Facts\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8739495798319328"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(annos_round_1['Advise Rater 1'], annos_round_1['Advise Rater 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46808510638297873\n",
      "Round 2:  0.7929203539823009\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(annos_round_1['Share Personal Information Rater 1'], annos_round_1['Share Personal Information Rater 2']))\n",
    "print(\"Round 2: \", cohen_kappa_score(annos_rater_1[\"Share Personal Information\"], annos_rater_2[\"Share Personal Information\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5575221238938053\n",
      "Round 2:  0.803030303030303\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(annos_round_1['Feeling Rater 1'], annos_round_1['Feeling Rater 2']))\n",
    "print(\"Round 2: \", cohen_kappa_score(annos_rater_1[\"Feeling\"], annos_rater_2[\"Feeling\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations of user behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_1_rater_1 = pd.read_csv(\"user_behaviour/Annotation_sample_round_1_rater_1.csv\")\n",
    "round_1_rater_2 = pd.read_csv(\"user_behaviour/Annotation_sample_round_1_rater_2.csv\")\n",
    "\n",
    "round_2_rater_1 = pd.read_csv(\"user_behaviour/Annotation_sample_round_2_rater_1.csv\")\n",
    "round_2_rater_2 = pd.read_csv(\"user_behaviour/Annotation_sample_round_2_rater_2.csv\")\n",
    "\n",
    "rater_1_overall = pd.concat([round_1_rater_1, round_2_rater_1])\n",
    "rater_2_overall = pd.concat([round_1_rater_2, round_2_rater_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Selina Max Reflective: 0.5142857142857142, \n",
      "\n",
      "      Cooperative: 0.8495575221238938 \n",
      "\n",
      "    Pre-informed: 0.5277777777777778\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Kappa Selina Max Reflective: {cohen_kappa_score(round_1_rater_1[\"Reflective\"], round_1_rater_2[\"Reflective\"])}, \\n\n",
    "      Cooperative: {cohen_kappa_score(round_1_rater_1[\"Cooperative\"], round_1_rater_2[\"Cooperative\"])} \\n\n",
    "    Pre-informed: {cohen_kappa_score(round_1_rater_1[\"Pre-informed/finds own solution\"], round_1_rater_2[\"Pre-informed/finds own solution\"])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Selina Max Round 2: \n",
      " Reflective: 0.6382978723404256, \n",
      "\n",
      "    Pre-informed: 0.8811188811188811\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Kappa Selina Max Round 2: \\n Reflective: {cohen_kappa_score(round_2_rater_1[\"Reflective\"], round_2_rater_2[\"Reflective\"])}, \\n\n",
    "    Pre-informed: {cohen_kappa_score(round_2_rater_1[\"Pre-informed/finds own solution\"], round_2_rater_2[\"Pre-informed/finds own solution\"])}\"\"\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
